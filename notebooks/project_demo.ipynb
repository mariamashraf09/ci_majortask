{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OAmU6itLcAQr"
      },
      "outputs": [],
      "source": [
        "#section 1 for gradient checking\n",
        "from network import Network\n",
        "from layers import Dense\n",
        "from activations import Sigmoid, Tanh\n",
        "from losses import MSE\n",
        "from optimizer import SGD\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Section 2: The XOR problem (training and results) using my libraries\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0,0],\n",
        "              [0,1],\n",
        "              [1,0],\n",
        "              [1,1]])\n",
        "\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# Build network: 2 → 4 → 1\n",
        "#2 input neurons (x1,x2), 4 hidden neurons, 1 output neuron\n",
        "model = Network([\n",
        "    Dense(2, 4),\n",
        "    #tanh provide non linearity to the 4 hidden neurons\n",
        "    Tanh(),\n",
        "    #takes 4 inputs from the hidden layer and outputs 0 or 1\n",
        "    Dense(4, 1),\n",
        "    #sigmoid outputs values between 0 and 1\n",
        "    Sigmoid()\n",
        "])\n",
        "\n",
        "loss_fn = MSE()\n",
        "optimizer = SGD(lr=0.1)\n",
        "\n",
        "# Training loop, 5000 iterations\n",
        "for epoch in range(5000):\n",
        "    pred = model.forward(X) #y^\n",
        "    loss = loss_fn.forward(y, pred)\n",
        "    grad = loss_fn.backward(y, pred) #dL/dy^\n",
        "    model.backward(grad) #backpropagate through the network\n",
        "    model.update(optimizer)\n",
        "#print loss each 50 iterations\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss = {loss:.5f}\")\n",
        "\n",
        "# Final predictions\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(model.forward(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hEgpPrrdHcr",
        "outputId": "664a3099-7f43-4b2b-cdb8-fdc142c0b730"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss = 0.28423\n",
            "Epoch 500, Loss = 0.16125\n",
            "Epoch 1000, Loss = 0.14480\n",
            "Epoch 1500, Loss = 0.13816\n",
            "Epoch 2000, Loss = 0.13470\n",
            "Epoch 2500, Loss = 0.13262\n",
            "Epoch 3000, Loss = 0.13124\n",
            "Epoch 3500, Loss = 0.13027\n",
            "Epoch 4000, Loss = 0.12955\n",
            "Epoch 4500, Loss = 0.12899\n",
            "\n",
            "Final Predictions:\n",
            "[[0.04326605]\n",
            " [0.4961586 ]\n",
            " [0.95139506]\n",
            " [0.50608851]]\n"
          ]
        }
      ]
    }
  ]
}